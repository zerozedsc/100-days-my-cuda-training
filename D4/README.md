# ğŸš€ CUDA 100-Day Challenge â€“ Day 4  

## **Memory Access Optimization: Coalesced Global Memory & Shared Memory Bank Conflicts**  

## **Objective**  
On **Day 4**, I focused on optimizing **CUDA memory access patterns** to improve performance.  
The goal was to:  
- Compare **coalesced vs. uncoalesced** global memory access.  
- Analyze **optimized shared memory** vs. **bank conflict-prone shared memory**.  
- Measure execution times to **quantify the impact of memory access patterns**.  

---

## **1. Understanding Global Memory Access Optimization**  
### **What I Did:**  
Global memory access efficiency is **crucial for CUDA performance**.  

ğŸ”¹ **Coalesced Access (Fast âœ…)**: Ensures **consecutive threads** access **consecutive memory addresses**, leading to efficient memory transactions.  
ğŸ”¹ **Uncoalesced Access (Slow âŒ)**: Introduces **strided access patterns**, causing inefficient memory transactions and reducing performance.  

ğŸ“Œ **Key Takeaways:**  
- **Coalesced memory access** results in **fewer memory transactions** and improved bandwidth utilization.  
- **Uncoalesced memory access** forces **multiple slow memory fetches**, leading to performance degradation.  

---

## **2. Understanding Shared Memory Bank Conflicts**  
### **What I Did:**  
Shared memory **allows faster memory access**, but **bank conflicts** can reduce efficiency.  

ğŸ”¹ **Optimized Shared Memory (Fast âœ…)**: Ensures each thread accesses a **different memory bank**, avoiding serialization delays.  
ğŸ”¹ **Bank Conflict-Prone Shared Memory (Slow âŒ)**: Multiple threads access the **same memory bank**, causing conflicts and **forcing sequential memory transactions**.  

ğŸ“Œ **Key Takeaways:**  
- **Shared memory is as fast as registers when optimized correctly**.  
- **Padding shared memory** (`BLOCK_SIZE + 1`) **prevents bank conflicts**.  

---

## **3. Mathematical Models & Algorithms for Memory Optimization**  

### **ğŸ“Œ Memory Coalescing Efficiency Equation**  
$$
\eta_c = \frac{\text{useful transactions}}{\text{total memory transactions}}
$$
- **Ideal case (Fully Coalesced Access)**: $\eta_c = 1$ (100% efficient).  
- **Worst case (Uncoalesced Access)**: $\eta_c \approx \frac{1}{32}$ (only 1 out of 32 transactions is useful).  

### **ğŸ“Œ Bank Conflict Penalty Equation**  
$$
\beta = \text{Number of memory transactions per warp}
$$
- **If $\beta = 1$**: **No conflict (Fast execution)** âœ…  
- **If $\beta > 1$**: **Conflicts occur (Slow execution)** âŒ  

### **ğŸ“Œ Algorithm for Coalesced Global Memory Access**
1. Ensure **thread `i` accesses memory location `i`**.  
2. Use **stride-1 access patterns** in row-major order.  
3. Align memory access to **128-byte segments** (for 32 threads in a warp).  

### **ğŸ“Œ Algorithm for Avoiding Bank Conflicts**
1. Ensure each **thread accesses a different memory bank**.  
2. **Add padding** to avoid conflicts:  
   ```cpp
   __shared__ float sharedMem[BLOCK_SIZE][BLOCK_SIZE + 1];  // +1 to prevent conflicts
3. Access memory in strided patterns that do not overlap.

## **3. Code Overview & File Structure**  

### **Files Used:**  
ğŸ“Œ **`main.cu`** â€“ Implements memory access optimizations for global and shared memory.  

### **Memory Type Breakdown:**  
- **Coalesced Global Memory** âœ… â€“ Ensures efficient memory access.  
- **Uncoalesced Global Memory** âŒ â€“ Strided access pattern leads to performance degradation.  
- **Optimized Shared Memory** âœ… â€“ Eliminates bank conflicts.  
- **Bank Conflict-Prone Shared Memory** âŒ â€“ Causes serialization of memory transactions.  

---

## **4. Observing Execution Times**  
### **Output Results (Measured Execution Time in ms)**
| Memory Access Type | Execution Time (ms) | Performance vs. Coalesced |
|----------------------|----------------|------------------|
| **Coalesced Global Memory** âœ… | **0.565 ms** | **Baseline (Fastest)** |
| **Uncoalesced Global Memory** âŒ | **1.862 ms** | **3.30x slower** |
| **Optimized Shared Memory** âœ… | **1.242 ms** | **2.20x slower** |
| **Bank Conflict-Prone Shared Memory** âŒ | **2.279 ms** | **1.84x slower than optimized shared memory** |

### **ğŸš€ Key Insights**
âœ… **Uncoalesced global memory is ~3.3x slower** than coalesced memory due to inefficient memory transactions.  
âœ… **Optimized shared memory is still ~2.2x slower** than coalesced global memory, likely due to synchronization overhead.  
âœ… **Bank conflict-prone shared memory is ~1.84x slower** than optimized shared memory, proving that **bank conflicts impact performance significantly**.  

---

## **5. Key Learnings from Day 4**  
âœ… **Coalesced memory access is the most efficient method** for utilizing global memory.  
âœ… **Shared memory must be structured properly** to avoid **bank conflicts** and maximize speed.  
âœ… **Even optimized shared memory can have synchronization overhead**, but is **still faster than global memory** in some cases.  
âœ… **Adding simple memory alignment (padding) significantly improves shared memory performance**.  

---

## **6. Next Steps (Day 5)**  
ğŸ”¹ Implement **parallel reduction** using shared memory.  
ğŸ”¹ Optimize **memory coalescing and bank conflict avoidance** further.  
ğŸ”¹ Experiment with **CUDA memory profiling tools (NVIDIA Nsight Compute, nvprof)**. 